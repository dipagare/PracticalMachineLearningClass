{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 04 - Linear Regression\n",
    "\n",
    "by [Alejandro Correa Bahnsen](albahnsen.com/)\n",
    "\n",
    "version 0.1, Feb 2016\n",
    "\n",
    "## Part of the class [Practical Machine Learning](https://github.com/albahnsen/PracticalMachineLearningClass)\n",
    "\n",
    "\n",
    "\n",
    "This notebook is licensed under a [Creative Commons Attribution-ShareAlike 3.0 Unported License](http://creativecommons.org/licenses/by-sa/3.0/deed.en_US). Special thanks goes to [Kevin Markham](https://github.com/justmarkham)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from matplotlib import cm\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Test Dataset\n",
    "\n",
    "data = pd.read_csv('houses_portland.csv')\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "y = data[' price'].values\n",
    "X = data['area'].values\n",
    "plt.scatter(X, y)\n",
    "plt.xlabel('Area')\n",
    "plt.ylabel('Price')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normalize data\n",
    "\n",
    "## $$ x = \\frac{x -\\overline x}{\\sigma_x} $$ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "y_mean, y_std = y.mean(), y.std()\n",
    "X_mean, X_std = X.mean(), X.std()\n",
    "\n",
    "y = (y - y_mean)/ y_std\n",
    "X = (X - X_mean)/ X_std\n",
    "\n",
    "plt.scatter(X, y)\n",
    "plt.xlabel('Area')\n",
    "plt.ylabel('Price')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Form of linear regression\n",
    "\n",
    "## $$h_\\beta(x) = \\beta_0 + \\beta_1x_1 + \\beta_2x_2 + ... + \\beta_nx_n$$\n",
    "\n",
    "- $h_\\beta(x)$ is the response\n",
    "- $\\beta_0$ is the intercept\n",
    "- $\\beta_1$ is the coefficient for $x_1$ (the first feature)\n",
    "- $\\beta_n$ is the coefficient for $x_n$ (the nth feature)\n",
    "\n",
    "The $\\beta$ values are called the **model coefficients**:\n",
    "\n",
    "- These values are estimated (or \"learned\") during the model fitting process using the **least squares criterion**.\n",
    "- Specifically, we are find the line (mathematically) which minimizes the **sum of squared residuals** (or \"sum of squared errors\").\n",
    "- And once we've learned these coefficients, we can use the model to predict the response.\n",
    "\n",
    "![Estimating coefficients](https://raw.githubusercontent.com/justmarkham/DAT8/master/notebooks/images/estimating_coefficients.png)\n",
    "\n",
    "In the diagram above:\n",
    "\n",
    "- The black dots are the **observed values** of x and y.\n",
    "- The blue line is our **least squares line**.\n",
    "- The red lines are the **residuals**, which are the vertical distances between the observed values and the least squares line."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Cost function\n",
    "\n",
    "The goal became to estimate the parameters $\\beta$ that minimisse the sum of squared residuals\n",
    "\n",
    "## $$J(\\beta_0, \\beta_1)=\\frac{1}{2n}\\sum_{i=1}^n (h_\\beta(x_i)-y_i)^2$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# create X and y\n",
    "n_samples = X.shape[0]\n",
    "X_ = np.c_[np.ones(n_samples), X]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets suppose the following betas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "beta_ini = np.array([-1, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# h\n",
    "def lr_h(beta,x):\n",
    "    return np.dot(beta, x.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# scatter plot\n",
    "plt.scatter(X, y)\n",
    "\n",
    "# Plot the linear regression\n",
    "x = np.c_[np.ones(2), [X.min(), X.max()]]\n",
    "plt.plot(x[:, 1], lr_h(beta_ini, x), 'r', lw=5)\n",
    "plt.xlabel('Area')\n",
    "plt.ylabel('Price')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets calculate the error of such regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Cost function\n",
    "def lr_cost_func(beta, x, y):\n",
    "    # Can be vectorized\n",
    "    res = 0\n",
    "    for i in range(x.shape[0]):\n",
    "        res += (lr_h(beta,x[i, :]) - y[i]) ** 2\n",
    "    res *= 1 / (2*x.shape[0])\n",
    "    return res\n",
    "lr_cost_func(beta_ini, X_, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Understanding the cost function\n",
    "\n",
    "Lets see how the cost function looks like for different values of $\\beta$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "beta0 = np.arange(-15, 20, 1)\n",
    "beta1 = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cost_func=[]\n",
    "for beta_0 in beta0:\n",
    "    cost_func.append(lr_cost_func(np.array([beta_0, beta1]), X_, y) )\n",
    "\n",
    "plt.plot(beta0, cost_func)\n",
    "plt.xlabel('beta_0')\n",
    "plt.ylabel('J(beta)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "beta0 = 0\n",
    "beta1 = np.arange(-15, 20, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cost_func=[]\n",
    "for beta_1 in beta1:\n",
    "    cost_func.append(lr_cost_func(np.array([beta0, beta_1]), X_, y) )\n",
    "\n",
    "plt.plot(beta1, cost_func)\n",
    "plt.xlabel('beta_1')\n",
    "plt.ylabel('J(beta)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Analyzing both at the same time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "beta0 = np.arange(-5, 7, 0.2)\n",
    "beta1 = np.arange(-5, 7, 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cost_func = pd.DataFrame(index=beta0, columns=beta1)\n",
    "\n",
    "for beta_0 in beta0:\n",
    "    for beta_1 in beta1:\n",
    "        cost_func.ix[beta_0, beta_1] = lr_cost_func(np.array([beta_0, beta_1]), X_, y)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "betas = np.transpose([np.tile(beta0, beta1.shape[0]), np.repeat(beta1, beta0.shape[0])])\n",
    "fig = plt.figure(figsize=(10, 10))\n",
    "ax = fig.gca(projection='3d')\n",
    "ax.plot_trisurf(betas[:, 0], betas[:, 1], cost_func.T.values.flatten(), cmap=cm.jet, linewidth=0.1)\n",
    "ax.set_xlabel('beta_0')\n",
    "ax.set_ylabel('beta_1')\n",
    "ax.set_zlabel('J(beta)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It can also be seen as a contour plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "contour_levels = [0, 0.1, 0.25, 0.5, 0.75, 1, 1.5, 2, 3, 4, 5, 7, 10, 12, 15, 20]\n",
    "plt.contour(beta0, beta1, cost_func.T.values, contour_levels)\n",
    "plt.xlabel('beta_0')\n",
    "plt.ylabel('beta_1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets understand how different values of betas are observed on the contour plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "betas = np.array([[0, 0],\n",
    "                 [-1, -1],\n",
    "                 [-5, 5],\n",
    "                 [3, -2]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for beta in betas:\n",
    "    print('\\n\\nLinear Regression with betas ', beta)\n",
    "    f, (ax1, ax2) = plt.subplots(1,2, figsize=(12, 6))\n",
    "    ax2.contour(beta0, beta1, cost_func.T.values, contour_levels)\n",
    "    ax2.set_xlabel('beta_0')\n",
    "    ax2.set_ylabel('beta_1')\n",
    "    ax2.scatter(beta[0], beta[1], s=50)\n",
    "\n",
    "    # scatter plot\n",
    "    ax1.scatter(X, y)\n",
    "\n",
    "    # Plot the linear regression\n",
    "    x = np.c_[np.ones(2), [X.min(), X.max()]]\n",
    "    ax1.plot(x[:, 1], lr_h(beta, x), 'r', lw=5)\n",
    "    ax1.set_xlabel('Area')\n",
    "    ax1.set_ylabel('Price')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient descent\n",
    "\n",
    "Have some function $J(\\beta_0, \\beta_1)$\n",
    "\n",
    "Want $\\min_{\\beta_0, \\beta_1}J(\\beta_0, \\beta_1)$\n",
    "\n",
    "Process:\n",
    "\n",
    "* Start with some $\\beta_0, \\beta_1$\n",
    "\n",
    "* Keep changing $\\beta_0, \\beta_1$ to reduce $J(\\beta_0, \\beta_1)$\n",
    "until hopefully end up at a minimum"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient descent algorithm\n",
    "\n",
    "Repeat until convergence{\n",
    "\n",
    "## $$ \\beta_j := \\beta_j - \\alpha \\frac{\\partial }{\\partial \\beta_j} J(\\beta_0, \\beta_1)$$\n",
    "\n",
    "\n",
    "}\n",
    "\n",
    "while simuntaneously update j=0 and j=1\n",
    "\n",
    "$\\alpha$ is refered as the learning rate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the particular case of linear regression with one variable and one intercept the gradient is calculated as:\n",
    "\n",
    "\n",
    "### $$\\frac{\\partial }{\\partial \\beta_j} J(\\beta_0, \\beta_1) = \\frac{\\partial }{\\partial \\beta_j} \\frac{1}{2n}\\sum_{i=1}^n (h_\\beta(x_i)-y_i)^2$$\n",
    "\n",
    "### $$\\frac{\\partial }{\\partial \\beta_j} J(\\beta_0, \\beta_1) = \\frac{\\partial }{\\partial \\beta_j} \\frac{1}{2n}\\sum_{i=1}^n (\\beta_0 + \\beta_1x_i-y_i)^2$$\n",
    "\n",
    "### $ j = 0: \\frac{\\partial }{\\partial \\beta_0} =  \\frac{1}{n}\\sum_{i=1}^n (\\beta_0 + \\beta_1x_i-y_i)$\n",
    "\n",
    "### $ j = 1: \\frac{\\partial }{\\partial \\beta_1} =  \\frac{1}{n}\\sum_{i=1}^n (\\beta_0 + \\beta_1x_i-y_i) \\cdot x_i$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient descent algorithm\n",
    "\n",
    "Repeat until convergence{\n",
    "\n",
    "### $ \\beta_0 := \\beta_0- \\alpha  \\frac{1}{n}\\sum_{i=1}^n (\\beta_0 + \\beta_1x_i-y_i)$\n",
    "\n",
    "### $ \\beta_1 := \\beta_1- \\alpha   \\frac{1}{n}\\sum_{i=1}^n (\\beta_0 + \\beta_1x_i-y_i) \\cdot x_i$\n",
    "}\n",
    "\n",
    "simultaneously!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# gradient calculation\n",
    "beta_ini = np.array([0, 0.855])\n",
    "\n",
    "def gradient(beta, x, y):\n",
    "    # Not vectorized\n",
    "    gradient_0  = 1 / x.shape[0] * ((lr_h(beta, x) - y).sum())\n",
    "    gradient_1  = 1 / x.shape[0] * ((lr_h(beta, x) - y)* x[:, 1]).sum()\n",
    "\n",
    "    return np.array([gradient_0, gradient_1])\n",
    "\n",
    "gradient(beta_ini, X_, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient descent algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def gradient_descent(x, y, beta_ini, alpha, iters): \n",
    "    betas = np.zeros((iters, beta_ini.shape[0] + 1))\n",
    "\n",
    "    beta = beta_ini\n",
    "    for iter_ in range(iters):\n",
    "\n",
    "        betas[iter_, :-1] = beta\n",
    "        betas[iter_, -1] = lr_cost_func(beta, x, y)\n",
    "        beta -= alpha * gradient(beta, x, y)\n",
    "        \n",
    "    return betas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "iters = 100\n",
    "alpha = 0.05\n",
    "beta_ini = np.array([-4., -4.])\n",
    "\n",
    "betas =  gradient_descent(X_, y, beta_ini, alpha, iters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets see the evolution of the cost per iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.plot(range(iters), betas[:, -1])\n",
    "plt.xlabel('iteration')\n",
    "plt.ylabel('J(beta)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Understanding what it is doing in each iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "betas_ = betas[range(0, iters, 10), :-1]\n",
    "for i, beta in enumerate(betas_):\n",
    "    print('\\n\\nLinear Regression with betas ', beta)\n",
    "    f, (ax1, ax2) = plt.subplots(1,2, figsize=(12, 6))\n",
    "    ax2.contour(beta0, beta1, cost_func.T.values, contour_levels)\n",
    "    ax2.set_xlabel('beta_0')\n",
    "    ax2.set_ylabel('beta_1')\n",
    "    ax2.scatter(beta[0], beta[1], c='r', s=50)\n",
    "    \n",
    "    if i > 0:\n",
    "        for beta_ in betas_[:i]:\n",
    "            ax2.scatter(beta_[0], beta_[1], s=50)\n",
    "\n",
    "    # scatter plot\n",
    "    ax1.scatter(X, y)\n",
    "\n",
    "    # Plot the linear regression\n",
    "    x = np.c_[np.ones(2), [X.min(), X.max()]]\n",
    "    ax1.plot(x[:, 1], lr_h(beta, x), 'r', lw=5)\n",
    "    ax1.set_xlabel('Area')\n",
    "    ax1.set_ylabel('Price')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Estimated Betas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "betas[-1, :-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normal equations (aka OLS)\n",
    "\n",
    "## $$ \\beta = (X^T X)^{-1} X^T Y $$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "beta = np.dot(np.linalg.inv(np.dot(X_.T, X_)),np.dot(X_.T, y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "beta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Estimating the regression using sklearn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using OLS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# import\n",
    "from sklearn.linear_model import LinearRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Initialize\n",
    "linreg = LinearRegression(fit_intercept=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Fit\n",
    "linreg.fit(X_, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "linreg.coef_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using (Stochastic) Gradient Descent*\n",
    "\n",
    "*Differs from normal gradient descent by updating the weights with each example. This converges faster for large datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# import\n",
    "from sklearn.linear_model import SGDRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Initialize\n",
    "linreg2 = SGDRegressor(fit_intercept=False, n_iter=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Fit\n",
    "linreg2.fit(X_, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "linreg2.coef_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comparing OLS and GD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "|Gradient Descent|Normal Equation|\n",
    "| :------------- | :------------- |\n",
    "| Need to choose $\\alpha$| No need to choose $\\alpha$|\n",
    "|Needs many iterations | Don't need to iterate|\n",
    "|Works weel even when $k$ is large |Slow if $k$ is very large |\n",
    "||Need to compute $(X^TX)^{-1}$|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear regression with multiple variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets create a new freature $ area^2 $"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data['area2'] = data['area'] ** 2\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Notation review"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " * n = n_samples = number of examples\n",
    " * k = number of features\n",
    " * y = price\n",
    " * $x^{(i)}$ = features of the $i$ example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "i = 2\n",
    "data.ix[2, ['area', 'area2']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* $x_j^{(i)}$ = value of the $j$ feature of the $i$ example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "i = 2\n",
    "j = 2\n",
    "data.ix[2, 'area2']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hypothesis:\n",
    "\n",
    "* Previously:\n",
    "\n",
    "### $$ h_\\beta(x) = \\beta_0 + \\beta_1 x_1 $$\n",
    "\n",
    "where $x_1$ = area\n",
    "\n",
    "* New:\n",
    "\n",
    "### $$ h_\\beta(x) = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2$$\n",
    "where $x_2$ = $area^2$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create new matrix X and scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X = data[['area', 'area2']].values\n",
    "X[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "ss = StandardScaler(with_mean=True, with_std=True)\n",
    "ss.fit(X.astype(np.float))\n",
    "X = ss.transform(X.astype(np.float))\n",
    "ss.mean_, ss.scale_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_ = np.c_[np.ones(n_samples), X]\n",
    "X_[0:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Cost function\n",
    "\n",
    "The goal became to estimate the parameters $\\beta$ that minimize the sum of squared residuals\n",
    "\n",
    "### $$J(\\beta)=\\frac{1}{2n}\\sum_{i=1}^n (h_\\beta(x^{(i)})-y_i)^2$$\n",
    "\n",
    "\n",
    "### $$h_\\beta(x^{(i)}) = \\sum_{j=0}^k \\beta_j  x_j^{(i)}$$\n",
    "\n",
    "\n",
    "### $$J(\\beta)=\\frac{1}{2n}\\sum_{i=1}^n \\left( \\left( \\sum_{j=0}^k \\beta_j  x_j^{(i)}\\right) -y_i \\right)^2$$\n",
    "\n",
    "Note that $x^0$ is refering to the column of ones"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient descent algorithm\n",
    "\n",
    "Repeat until convergence{\n",
    "\n",
    "### $$ \\beta_j := \\beta_j - \\alpha \\frac{\\partial }{\\partial \\beta_j} J(\\beta)$$\n",
    "\n",
    "\n",
    "}\n",
    "\n",
    "while simuntaneously update j=0..k\n",
    "\n",
    "$\\alpha$ is refered as the learning rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "beta_ini = np.array([0., 0., 0.])\n",
    "\n",
    "# gradient calculation\n",
    "def gradient(beta, x, y):\n",
    "    return 1 / x.shape[0] * np.dot((lr_h(beta, x) - y).T, x)\n",
    "\n",
    "gradient(beta_ini, X_, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "beta_ini = np.array([0., 0., 0.])\n",
    "alpha = 0.005\n",
    "iters = 100\n",
    "betas = gradient_descent(X_, y, beta_ini, alpha, iters)\n",
    "\n",
    "# Print iteration vs J\n",
    "plt.plot(range(iters), betas[:, -1])\n",
    "plt.xlabel('iteration')\n",
    "plt.ylabel('J(beta)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aparently the cost function is not converging \n",
    "\n",
    "Lets change alpha and increase the number of iterations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "beta_ini = np.array([0., 0., 0.])\n",
    "alpha = 0.5\n",
    "iters = 1000\n",
    "betas = gradient_descent(X_, y, beta_ini, alpha, iters)\n",
    "\n",
    "# Print iteration vs J\n",
    "plt.plot(range(iters), betas[:, -1])\n",
    "plt.xlabel('iteration')\n",
    "plt.ylabel('J(beta)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print('betas using gradient descent\\n', betas[-1, :-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using the normal equations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "betas_ols = np.dot(np.linalg.inv(np.dot(X_.T, X_)),np.dot(X_.T, y))\n",
    "betas_ols"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Difference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "betas_ols - betas[-1, :-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Making predictions\n",
    "\n",
    "Predict the price when the area is 3000\n",
    "\n",
    "Note: remeber the matrix X is scaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x = np.array([3000., 3000.**2])\n",
    "\n",
    "# scale\n",
    "x_scaled = ss.transform(x.reshape(1, -1))\n",
    "x_ = np.c_[1, x_scaled]\n",
    "x_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "y_pred = lr_h(betas_ols, x_)\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "y_pred = y_pred * y_std + y_mean\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import SGDRegressor\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "clf1 = LinearRegression()\n",
    "clf2 = SGDRegressor(n_iter=10000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When using sklearn there is no need to create the intercept\n",
    "\n",
    "Also sklearn works with pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "clf1.fit(data[['area', 'area2']], data[' price'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "clf2.fit(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Making predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "clf1.predict(x.reshape(1, -1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "clf2.predict(x_scaled.reshape(1, -1)) * y_std + y_mean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation metrics for regression problems\n",
    "\n",
    "Evaluation metrics for classification problems, such as **accuracy**, are not useful for regression problems. We need evaluation metrics designed for comparing **continuous values**.\n",
    "\n",
    "Here are three common evaluation metrics for regression problems:\n",
    "\n",
    "**Mean Absolute Error** (MAE) is the mean of the absolute value of the errors:\n",
    "\n",
    "$$\\frac 1n\\sum_{i=1}^n|y_i-\\hat{y}_i|$$\n",
    "\n",
    "**Mean Squared Error** (MSE) is the mean of the squared errors:\n",
    "\n",
    "$$\\frac 1n\\sum_{i=1}^n(y_i-\\hat{y}_i)^2$$\n",
    "\n",
    "**Root Mean Squared Error** (RMSE) is the square root of the mean of the squared errors:\n",
    "\n",
    "$$\\sqrt{\\frac 1n\\sum_{i=1}^n(y_i-\\hat{y}_i)^2}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_pred = clf1.predict(data[['area', 'area2']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "import numpy as np\n",
    "print('MAE:', metrics.mean_absolute_error(data[' price'], y_pred))\n",
    "print('MSE:', metrics.mean_squared_error(data[' price'], y_pred))\n",
    "print('RMSE:', np.sqrt(metrics.mean_squared_error(data[' price'], y_pred)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comparing these metrics:\n",
    "\n",
    "- **MAE** is the easiest to understand, because it's the average error.\n",
    "- **MSE** is more popular than MAE, because MSE \"punishes\" larger errors, which tends to be useful in the real world.\n",
    "- **RMSE** is even more popular than MSE, because RMSE is interpretable in the \"y\" units.\n",
    "\n",
    "All of these are **loss functions**, because we want to minimize them.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comparing linear regression with other models\n",
    "\n",
    "Advantages of linear regression:\n",
    "\n",
    "- Simple to explain\n",
    "- Highly interpretable\n",
    "- Model training and prediction are fast\n",
    "- No tuning is required (excluding regularization)\n",
    "- Features don't need scaling\n",
    "- Can perform well with a small number of observations\n",
    "- Well-understood\n",
    "\n",
    "Disadvantages of linear regression:\n",
    "\n",
    "- Presumes a linear relationship between the features and the response\n",
    "- Performance is (generally) not competitive with the best supervised learning methods due to high bias\n",
    "- Can't automatically learn feature interactions"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
